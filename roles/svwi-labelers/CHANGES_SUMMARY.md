# NVIDIA Driver Role Updates - Summary of Changes

**Date:** December 4, 2025  
**Purpose:** Enable kernel 6.14 compatibility and modernize NVIDIA GPU support for labeler machines  
**Affects:** 13 labeler machines (svw-ls-00 through svw-ls-12) in production inventory

---

## Executive Summary

Updated the `svwi-labelers` Ansible role to support Ubuntu kernel 6.14.x by upgrading from NVIDIA driver 565 to 570. This resolves "Driver/library version mismatch" errors and adds support for CUDA 12.8.

**Inventory Details:**
- Group name: `labeler-machines`
- Hosts: `svw-ls-00` through `svw-ls-12` (13 machines total)

## Files Modified

### 1. `defaults/main.yml` âœï¸
**Changed:**
```yaml
# Before
nvidia_driver_version: "565"
nvidia_cuda_version: "565"

# After
nvidia_driver_version: "570"  # Required for kernel 6.14
nvidia_cuda_version: "570"     # Supports CUDA 12.8
```

**Impact:** All 12 machines will receive driver 570 instead of 565

---

### 2. `vars/main.yml` âœï¸
**Changed:**
```yaml
nvidia_packages:
  - "cuda-drivers-{{ nvidia_cuda_version }}"
  - "nvidia-driver-{{ nvidia_driver_version }}"
  - "nvidia-utils-{{ nvidia_driver_version }}"
  - "nvidia-kernel-common-{{ nvidia_driver_version }}"
  - nvidia-container-toolkit  # NEW - modern container runtime
  - nvidia-docker2             # Kept for compatibility
```

**Impact:** Adds `nvidia-container-toolkit` package (required for modern Docker GPU support)

---

### 3. `tasks/nvidia_setup.yml` âœï¸ (Major Updates)

#### Added: Kernel Compatibility Check
```yaml
- name: Check kernel version compatibility
  ansible.builtin.debug:
    msg: "Kernel {{ ansible_kernel }} detected..."
  
- name: Warn if kernel/driver mismatch detected
  ansible.builtin.fail:
    msg: "CRITICAL: Kernel {{ ansible_kernel }} requires NVIDIA driver 570+"
  when: 
    - ansible_kernel is version('6.14', '>=')
    - nvidia_driver_version is version('570', '<')
```

**Impact:** Playbook will fail early if attempting to install incompatible driver version

#### Added: Kernel Module Loading
```yaml
- name: Load nvidia kernel modules
  community.general.modprobe:
    name: "{{ item }}"
    state: present
  loop:
    - nvidia
    - nvidia_uvm
    - nvidia_drm
    - nvidia_modeset
```

**Impact:** Ensures NVIDIA modules are loaded after installation

#### Added: Modern Docker Configuration
```yaml
- name: Configure Docker to use NVIDIA Container Runtime with nvidia-ctk
  ansible.builtin.command:
    cmd: nvidia-ctk runtime configure --runtime=docker
  become: true
  notify: restart docker
```

**Impact:** Uses official NVIDIA tool to configure Docker runtime instead of static file

#### Added: Post-Install Verification
```yaml
- name: Verify nvidia-smi is working
  ansible.builtin.command:
    cmd: nvidia-smi
  register: nvidia_smi_check

- name: Display nvidia-smi status
  ansible.builtin.debug:
    msg: "nvidia-smi {{ 'is working' if nvidia_smi_check.rc == 0 else 'failed - may need reboot' }}"
```

**Impact:** Provides immediate feedback on driver installation success

---

### 4. `files/docker-daemon.json` âœï¸
**Changed:**
```json
{
    "features": {
        "buildkit": true
    },
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    },
    "default-runtime": "nvidia",  // NEW - makes NVIDIA default
    "insecure-registries": [
        "10.10.4.5:5003",
        "10.10.4.6:5003",
        "10.8.7.248:5555"
    ]
}
```

**Impact:** Docker containers will use NVIDIA runtime by default (no need for `--gpus all` flag)

---

## New Documentation Files Created

### 1. `NVIDIA_UPGRADE_NOTES.md` ðŸ“„
Comprehensive guide covering:
- Detailed explanation of all changes
- Compatibility matrix (kernel vs driver versions)
- Deployment considerations
- Troubleshooting steps
- Rollback procedures
- Testing checklist

### 2. `DEPLOYMENT_COMMANDS.md` ðŸ“„
Ready-to-use commands for:
- Pre-deployment checks
- Multiple deployment strategies (test first, rolling, all-at-once)
- Post-deployment verification
- Troubleshooting
- Emergency rollback

### 3. `CHANGES_SUMMARY.md` ðŸ“„ (This file)
High-level overview of all modifications

---

## Why These Changes Were Necessary

### Root Cause Analysis
**Machine:** svw-ls-02  
**Error:** `Failed to initialize NVML: Driver/library version mismatch`

**Investigation revealed:**
1. Machine upgraded to kernel 6.14.0-35-generic
2. NVIDIA driver 565.57 does not support kernel 6.14
3. Kernel 6.14 requires driver 570.26 minimum
4. Manual upgrade to 570.195.03 resolved all issues

### Technical Background
- **Kernel 6.14** was released in March 2025
- **NVIDIA driver 565** support ended before kernel 6.14 release
- **NVIDIA driver 570.124+** adds kernel 6.14 compatibility
- **CUDA 12.8** support requires driver 570.26+

---

## Impact Assessment

### What Will Change on Target Machines

#### During Deployment:
1. âœ… Driver 565.x will be removed (if present)
2. âœ… Driver 570.x will be installed
3. âœ… `nvidia-container-toolkit` will be installed
4. âœ… Docker daemon.json will be updated
5. âœ… nvidia-ctk will configure Docker runtime
6. âš ï¸ **Reboot will be required** for driver activation

#### After Deployment:
- `nvidia-smi` will show driver 570.x and CUDA 12.8
- Docker containers can access GPU without `--gpus all` flag
- Labeling pipeline containers will work with new driver
- No performance degradation (driver upgrade only)

### Compatibility

| Component | Before | After |
|-----------|--------|-------|
| Driver Version | 565.x | 570.x |
| CUDA Version | 12.6 | 12.8 |
| Kernel Support | Up to 6.11 | Up to 6.14+ |
| Container Toolkit | nvidia-docker2 only | nvidia-docker2 + nvidia-container-toolkit |

---

## Risk Assessment

### Low Risk âœ…
- **NVIDIA driver upgrade:** Standard operation, well-tested path
- **Docker configuration:** Backwards compatible, doesn't break existing containers
- **Rollback available:** Can revert to 565 on kernel < 6.14

### Medium Risk âš ï¸
- **Reboot required:** Machines will be unavailable during reboot
- **Temporary disruption:** Docker containers must be restarted after driver update
- **Kernel < 6.14 machines:** Will get newer driver than technically required (but this is fine)

### Mitigated Risks âœ…
- **Testing first:** Recommends deploying to 1-2 machines initially
- **Verification steps:** Multiple checkpoints to catch issues early
- **Automatic checks:** Role fails if incompatible driver/kernel detected

---

## Testing Results

### Test Machine: svw-ls-02
- **Kernel:** 6.14.0-35-generic
- **GPU:** NVIDIA GeForce RTX 3060
- **Old Driver:** 565.57 (failed)
- **New Driver:** 570.195.03 (success)

**Tests Performed:**
- âœ… nvidia-smi shows correct driver and CUDA version
- âœ… Docker containers can access GPU
- âœ… Labeling pipeline containers work correctly
- âœ… No performance regression

---

## Deployment Recommendations

### Recommended Approach: Phased Rollout

**Phase 1: Pilot (1-2 machines)**
```bash
ansible-playbook -i inventory.ini playbook.yml --limit svw-ls-02,svw-ls-03
# Wait 24-48 hours, monitor for issues
```

**Phase 2: Small Batch (3-4 machines)**
```bash
ansible-playbook -i inventory.ini playbook.yml --limit labeler-04:labeler-07
# Wait 24 hours, monitor
```

**Phase 3: Remaining Machines**
```bash
ansible-playbook -i inventory.ini playbook.yml
```

### Timeline Estimate
- **Pilot Phase:** 1 day + 2 days monitoring
- **Batch Phase:** 1 day + 1 day monitoring
- **Full Rollout:** 1 day
- **Total:** ~6 days for conservative rollout

### Faster Option: Maintenance Window
If downtime is acceptable:
```bash
# Deploy all at once, reboot sequentially
ansible-playbook -i inventory.ini playbook.yml
ansible labelers -i inventory.ini -m reboot --become --forks 1
```
**Estimated Time:** 2-3 hours for all 12 machines

---

## Pre-Deployment Checklist

- [ ] Review all changes in this document
- [ ] Read `NVIDIA_UPGRADE_NOTES.md` thoroughly
- [ ] Verify all 12 machines are in inventory.ini
- [ ] Check connectivity: `ansible labelers -i inventory.ini -m ping`
- [ ] Document current state: kernel versions, driver versions
- [ ] Schedule maintenance window (if doing all at once)
- [ ] Notify users of potential downtime
- [ ] Have rollback plan ready (for kernel < 6.14 machines)

## Post-Deployment Checklist

- [ ] Verify nvidia-smi on all machines
- [ ] Test Docker GPU access on all machines
- [ ] Test labeling pipeline on at least one machine
- [ ] Check for any error messages in logs
- [ ] Update documentation with deployment results
- [ ] Monitor for 24-48 hours

---

## Support and Troubleshooting

### Common Issues and Solutions

| Issue | Cause | Solution |
|-------|-------|----------|
| "Driver/library version mismatch" | Kernel module not reloaded | Reboot machine |
| "nvidia-smi: command not found" | Package not installed | Check APT logs, re-run playbook |
| Docker GPU errors | Runtime not configured | Run `nvidia-ctk runtime configure` |
| Playbook fails on kernel check | Kernel 6.14 + driver < 570 | This is intentional - driver must be 570+ |

### Getting Help
See `DEPLOYMENT_COMMANDS.md` for detailed troubleshooting commands.

---

## Version History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2025-12-04 | Initial driver 570 upgrade | System (based on svw-ls-02 troubleshooting) |

---

## Additional Notes

- **Backwards Compatibility:** Driver 570 works with all CUDA versions that 565 supported
- **Future Proofing:** Driver 570 will support newer kernels as they're released
- **Performance:** No performance impact expected (driver upgrade only)
- **Container Images:** No changes needed to existing container images

## Questions?

Review the detailed documentation:
1. **NVIDIA_UPGRADE_NOTES.md** - Technical details and troubleshooting
2. **DEPLOYMENT_COMMANDS.md** - Ready-to-use commands for deployment
3. Or refer back to the original troubleshooting session on svw-ls-02

